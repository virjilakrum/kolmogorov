# DPO (Direct Preference Optimization) Configuration
# Extends base_config.yaml

_extends: base_config.yaml

dpo:
  beta: 0.1  # Temperature parameter controlling deviation from reference
  loss_type: "sigmoid"  # sigmoid, hinge, ipo, kto_pair
  label_smoothing: 0.0  # For conservative DPO (cDPO)
  max_length: 1024
  max_prompt_length: 512
  
  # Reference model settings
  ref_model: null  # If null, creates frozen copy of model
  precompute_ref_log_probs: false  # Set true for large datasets

training:
  output_dir: "./checkpoints/dpo"
  num_train_epochs: 1
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 5.0e-7  # Lower LR for DPO
  warmup_ratio: 0.1

data:
  dataset_name: null  # Will be provided at runtime
  # Expected format: {"prompt": "...", "chosen": "...", "rejected": "..."}
  # Or conversational: {"chosen": [{"role": "user", ...}], "rejected": [...]}

